---
title: PROJECTS
layout: content

---

### Select Projects
A selection of projects from my research and academia experience

##### Generative Pre-Training for Motion Capture Gesture Annotation
<font size=2>9/2021 - present<br>
 University of Minnesota - Applied Motion Laboratory and NeuroRehabilitation Across the Lifespan Laboratory<br></font>
 <img src="assets/headshot.png" align="left" alt="Headshot of Shelby wearing a white t-shirt and a green scarf." width="276" height="354" style="padding: 15px;"/>
<font size=4>We are developing a software tool for clinitians that allow deeper and more quantitative, objective, and efficient evaluation of bilateral coordination ability. This type of evaluation is used particularly in clinical rehabilitation for children with hemiparetic cerebral palsy but is applicable to any person with asymmetrical movement impairment. We use available optical motion capture and skeleton fitting algorithms to extract pose information, a variety of interactive data visualization techniques, and deep neural networks to automate information generation as much as possible so the clinitian can focus on using their expertise to develop treatment and care plans.</font>

{% include button.html text="More" link="https://david.darn.es" %}
<br>

##### Machine Learning for Optimizing Sampling Based Path Planning
<font size=2>7/2020 - present<br>
University of Minnesota - Applied Motion Lab<br></font>
<img src="assets/headshot.png" align="left" alt="Headshot of Shelby wearing a white t-shirt and a green scarf." width="276" height="354" style="padding: 15px;"/>
<font size=4>As an exploration into sampling-based path planning algorithms as well as machine learning, I am working on this ongoing project to develop new ways of harnessing neural networks to increase sampling efficiency for sampling-based path planning methods. For this project, I implemented a Rapidly Exploring Random Tree (RRT) algorithm, as well as it's close optimal cousin RRT*, which I used to generate data for my neural network to learn and evaluate on. This data included</font>

##### Projection Mapped Moss Map
<font size=2>6/2019 - 1/2020<br>
Downstream - Google<br></font>
<img src="assets/headshot.png" align="left" alt="Headshot of Shelby wearing a white t-shirt and a green scarf." width="276" height="354" style="padding: 15px;"/>
<font size=4>With my fantastic team at Downstream, we were able to build and install a lobby entry wall for the newly constructed 315 Hudson Google offices. This installation included 2 LED walls that flank projection on a physical map of Manhattan with preserved moss inlayed into the streets and waterways as well as an associated LED wall by the elevator banks that displays the same content. 
This native windows application was build in Downstream's custom framework, DS Cinder, which is extended from the open source</font>

##### Innovation Wall
<font size=2>6/2019 - 1/2020<br>
Downstream - Elekta<br></font>
<img src="assets/headshot.png" align="left" alt="Headshot of Shelby wearing a white t-shirt and a green scarf." width="276" height="354" style="padding: 15px;"/>
<font size=4>With my talented coworkers at Downstream, we installed an interactive digital timeline application built in Downstream's custom framework that extends from the open source library Cinder in the lobby of Elekta's corporate headquarters in London, England. This timeline is installed on a long horizontal LED wall with 2 motion tracking Orbbec Persee cameras mounted above the wall. These cameras have small computers inside where I installed a small app that splits the camera's view into a set number of "bins", or a 3D grid of space, the app then sends the information about which bins are occupied through network </font>

##### Low-Cost Optical Motion Capture System
<font size=2>8/2017 - 5/2018<br>
Lewis & Clark College - Department of Mathematical Sciences<br></font>
<img src="assets/headshot.png" align="left" alt="Headshot of Shelby wearing a white t-shirt and a green scarf." width="276" height="354" style="padding: 15px;"/>
<font size=4>I created a low-cost optical motion capture system which takes in two pieces of video footage of a distinct marker moving in 3D space (as well as information about the environment), and outputs two pieces of video footage with the marker tracked throughout. Additionally, this system provides a list of 3D vectors that contain either the actual 3D location of that marker in every frame or enough to further triangulate the best estimate. The input video footage must have the marker visible in every frame, not occluded by any object, and be taken at the same instant with two separate cameras, in different locations, watching the same scene. The environmental information required includes the 3D locations of four reference points, the 2D</font>

##### Individual Calibration of Redirected Walking Thresholds
<font size=2>6/2017 - 11/2017<br>
University of Southern California - Institute for Creative Technologies, Mixed Reality Lab<br></font>
<img src="assets/headshot.png" align="left" alt="Headshot of Shelby wearing a white t-shirt and a green scarf." width="276" height="354" style="padding: 15px;"/>
<font size=4>Coming soon...</font>



### Class Projects
A selection of projects from my classwork


##### Halloween Ghost Cloth Simulation
<font size=2>10/2020<br>
University of Minnesota - Animation and Planning in Games (course)<br></font>
<img src="assets/headshot.png" align="left" alt="Headshot of Shelby wearing a white t-shirt and a green scarf." width="276" height="354" style="padding: 15px;"/>
<font size=4>For a school project designed to explore cloth simulation, my project partner and I created a movable/interactive ghost with a spooky cloth cover. The simulation starts with an empty starry sky. Once played, a flat sheet falls down and you see it deform around an invisible sphere to create the shape of the ghost. The edges fall and and the cloth stretches a bit. We also added a pretty high level of air resistance so the falling sheet has a floaty effect. While this makes it look less realistic as cloth, it makes it</font>

